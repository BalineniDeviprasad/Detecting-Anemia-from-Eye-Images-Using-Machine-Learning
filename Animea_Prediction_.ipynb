{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obytPNIBTGBE",
        "outputId": "5e8b9474-97ae-402b-9cba-0dface655d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5cdRULH4Ggx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lioA_Ms3YJea"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FmwLDjMYaQz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "img = mpimg.imread(\"/content/drive/MyDrive/Eye dataset/Newimage/new1.jpg\")\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.title('Colour IMAGE')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwp9L_JRSviU"
      },
      "outputs": [],
      "source": [
        "# PRE-PROCESSING\n",
        "import numpy as np\n",
        "h1=224\n",
        "w1=224\n",
        "\n",
        "dimension = (w1, h1)\n",
        "resized_image = cv2.resize(img,(h1,w1))\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.title('RESIZED IMAGE')\n",
        "plt.imshow(resized_image)\n",
        "\n",
        "SP = np.shape(resized_image)\n",
        "try:\n",
        "\n",
        "    Red = resized_image[:,:,0]\n",
        "    Green = resized_image[:,:,1]\n",
        "    Blue = resized_image[:,:,2]\n",
        "\n",
        "\n",
        "\n",
        "    plt.imshow(Red)\n",
        "    plt.title('RED IMAGE')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.imshow(Green)\n",
        "    plt.title('GREEN IMAGE')\n",
        "    plt.show()\n",
        "\n",
        "    plt.imshow(Blue)\n",
        "    plt.title('BLUE IMAGE')\n",
        "    plt.show()\n",
        "    GRAY = resized_image[:,:,0]\n",
        "\n",
        "except:\n",
        "    GRAY = resized_image\n",
        "\n",
        "\n",
        "\n",
        "plt.imshow(GRAY)\n",
        "plt.title('GRAY IMAGE')\n",
        "plt.show()\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "\n",
        "# Apply a median filter\n",
        "median_filtered = cv2.medianBlur(GRAY, 5)  # You can adjust the kernel size (5 in this case)\n",
        "\n",
        "plt.imshow(median_filtered)\n",
        "plt.title('Median Filter IMAGE')\n",
        "plt.show()\n",
        "\n",
        "# --------------------\n",
        "\n",
        "#Resizing the image\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def resize_image(image, target_size=(224, 224)):\n",
        "    return cv2.resize(image, target_size)\n",
        "\n",
        "def normalize_image(image):\n",
        "    return image / 255.0  # Normalize pixel values to the range [0, 1]\n",
        "\n",
        "def convert_to_grayscale(image):\n",
        "    try:\n",
        "        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    except:\n",
        "        grayscale_image = image\n",
        "\n",
        "    return grayscale_image\n",
        "\n",
        "def equalize_histogram(image):\n",
        "    try:\n",
        "        equalized_image = cv2.equalizeHist(image)\n",
        "    except:\n",
        "        equalized_image = image\n",
        "    return equalized_image\n",
        "\n",
        "def apply_gaussian_blur(image, kernel_size=(5, 5)):\n",
        "    return cv2.GaussianBlur(image, kernel_size, 0)\n",
        "\n",
        "def augment_data(image):\n",
        "    # Example: Randomly flip horizontally with 50% probability\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = cv2.flip(image, 1)\n",
        "    return image\n",
        "\n",
        "# Example usage:\n",
        "original_image = img\n",
        "\n",
        "resized_image1 = resize_image(original_image)\n",
        "normalized_image = normalize_image(resized_image)\n",
        "grayscale_image = convert_to_grayscale(resized_image)\n",
        "equalized_image = equalize_histogram(grayscale_image)\n",
        "blurred_image = apply_gaussian_blur(resized_image)\n",
        "augmented_image = augment_data(resized_image)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_images(images, titles, rows=1, cols=None, figsize=(15, 5)):\n",
        "    if cols is None:\n",
        "        cols = len(images) // rows + (len(images) % rows > 0)\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i, (image, title) in enumerate(zip(images, titles), 1):\n",
        "        plt.subplot(rows, cols, i)\n",
        "        plt.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
        "        plt.title(title)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "image_titles = ['Original', 'Resized', 'Normalized', 'Grayscale', 'Equalized', 'Blurred', 'Augmented']\n",
        "images = [original_image, resized_image, normalized_image, grayscale_image, equalized_image, blurred_image, augmented_image]\n",
        "\n",
        "plot_images(images, image_titles, rows=1, figsize=(20, 10))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_images(images, titles, rows=1, cols=None, figsize=(15, 5)):\n",
        "    if cols is None:\n",
        "        cols = len(images) // rows + (len(images) % rows > 0)\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i, (image, title) in enumerate(zip(images, titles), 1):\n",
        "        plt.subplot(rows, cols, i)\n",
        "        plt.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
        "        plt.title(title)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "image_titles = ['Original', 'Resized', 'Normalized', 'Grayscale', 'Equalized', 'Blurred', 'Augmented']\n",
        "images = [original_image, resized_image, normalized_image, grayscale_image, equalized_image, blurred_image, augmented_image]\n",
        "\n",
        "plot_images(images, image_titles, rows=1, figsize=(20, 10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYyYfRN2TIM1"
      },
      "outputs": [],
      "source": [
        "from skimage.transform import rotate\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import data\n",
        "from skimage.color import label2rgb\n",
        "\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "import numpy as np\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Load the image\n",
        "image = GRAY.astype('uint8')\n",
        "\n",
        "# Calculate the GLCM\n",
        "d = 1  # Distance between pixels\n",
        "angles = [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4]  # Angles for co-occurrence matrix\n",
        "levels = 256  # Number of gray levels\n",
        "glcm = greycomatrix(image, [d], angles, levels=levels, symmetric=True, normed=True)\n",
        "\n",
        "# Calculate GLCM properties (example: contrast and energy)\n",
        "contrast = greycoprops(glcm, prop='contrast')\n",
        "\n",
        "energy = greycoprops(glcm, prop='energy')\n",
        "\n",
        "corr = greycoprops(glcm, prop='correlation')\n",
        "\n",
        "dissim = greycoprops(glcm, prop='dissimilarity')\n",
        "\n",
        "auto_correlation = np.sum(glcm[0, :, :, 0] * np.arange(levels) * np.arange(levels))\n",
        "\n",
        "homog = greycoprops(glcm, prop='homogeneity')\n",
        "\n",
        "# Calculate Sum Average\n",
        "sum_average = greycoprops(glcm, 'ASM')\n",
        "\n",
        "# Calculate Sum Variance\n",
        "sum_variance = greycoprops(glcm, 'contrast')\n",
        "\n",
        "# Calculate Sum Entropy\n",
        "sum_entropy = greycoprops(glcm, 'homogeneity')\n",
        "\n",
        "# Print or use the GLCM properties for texture analysis\n",
        "print(\"Contrast:\", contrast)\n",
        "print(\"Energy:\", energy)\n",
        "print(\"Correlation:\", corr)\n",
        "print(\"Dis-Similarity:\", dissim)\n",
        "print(\"Auto-Correlation:\", auto_correlation)\n",
        "print(\"Homoginity:\", homog)\n",
        "\n",
        "\n",
        "# Print or use the GLCM properties\n",
        "print(\"Sum Average:\", sum_average)\n",
        "print(\"Sum Variance:\", sum_variance)\n",
        "print(\"Sum Entropy:\", sum_entropy)\n",
        "\n",
        "temp_m1 = np.mean(GRAY)\n",
        "temp_st1 = np.std(GRAY)\n",
        "temp_vt1 = np.var(GRAY)\n",
        "temp_sk1 = np.mean(skew(GRAY))\n",
        "temp_md1 = np.median(GRAY)\n",
        "\n",
        "temp_m2 = np.mean(median_filtered)\n",
        "temp_st2 = np.std(median_filtered)\n",
        "temp_vt2 = np.var(median_filtered)\n",
        "temp_sk2 = np.mean(skew(median_filtered))\n",
        "temp_md2 = np.median(median_filtered)\n",
        "\n",
        "# Load the image\n",
        "image = GRAY\n",
        "median_filtered = cv2.medianBlur(image, 5)  # You can adjust the kernel size (5 in this case)\n",
        "\n",
        "temp_m1 = np.mean(image)\n",
        "temp_st1 = np.std(image)\n",
        "temp_vt1 = np.var(image)\n",
        "temp_sk1 = np.mean(skew(image))\n",
        "temp_md1 = np.median(image)\n",
        "\n",
        "temp_m2 = np.mean(median_filtered)\n",
        "temp_st2 = np.std(median_filtered)\n",
        "temp_vt2 = np.var(median_filtered)\n",
        "temp_sk2 = np.mean(skew(median_filtered))\n",
        "temp_md2 = np.median(median_filtered)\n",
        "\n",
        "\n",
        "# ==========================================================================\n",
        "\n",
        "# Feature Extraction\n",
        "\n",
        "\n",
        "\n",
        "# open the image\n",
        "\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "\n",
        "PATCH_SIZE = 21\n",
        "\n",
        "image = median_filtered\n",
        "image = (cv2.resize(image,(768,1024))*255).astype('uint8')\n",
        "\n",
        "# select some patches from foreground and background\n",
        "\n",
        "grass_locations = [(280, 454), (342, 223), (444, 192), (455, 455)]\n",
        "grass_patches = []\n",
        "for loc in grass_locations:\n",
        "    grass_patches.append(image[loc[0]:loc[0] + PATCH_SIZE,\n",
        "                               loc[1]:loc[1] + PATCH_SIZE])\n",
        "\n",
        "# select some patches from sky areas of the image\n",
        "sky_locations = [(38, 34), (139, 28), (37, 437), (145, 379)]\n",
        "sky_patches = []\n",
        "for loc in sky_locations:\n",
        "    sky_patches.append(image[loc[0]:loc[0] + PATCH_SIZE,\n",
        "                             loc[1]:loc[1] + PATCH_SIZE])\n",
        "\n",
        "# compute some GLCM properties each patch\n",
        "xs = []\n",
        "ys = []\n",
        "for patch in (grass_patches + sky_patches):\n",
        "    glcm = graycomatrix(patch, distances=[5], angles=[0], levels=256,symmetric=True)\n",
        "    xs.append(graycoprops(glcm, 'dissimilarity')[0, 0])\n",
        "    ys.append(graycoprops(glcm, 'correlation')[0, 0])\n",
        "\n",
        "# create the figure\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "# display original image with locations of patches\n",
        "ax = fig.add_subplot(3, 2, 1)\n",
        "ax.imshow(image, cmap=plt.cm.gray,\n",
        "          vmin=0, vmax=255)\n",
        "for (y, x) in grass_locations:\n",
        "    ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'gs')\n",
        "for (y, x) in sky_locations:\n",
        "    ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'bs')\n",
        "ax.set_xlabel('Original Image')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.axis('image')\n",
        "\n",
        "# for each patch, plot (dissimilarity, correlation)\n",
        "ax = fig.add_subplot(3, 2, 2)\n",
        "ax.plot(xs[:len(grass_patches)], ys[:len(grass_patches)], 'go',\n",
        "        label='Region 1')\n",
        "ax.plot(xs[len(grass_patches):], ys[len(grass_patches):], 'bo',\n",
        "        label='Region 2')\n",
        "ax.set_xlabel('Feature Index')\n",
        "ax.set_ylabel('Feature Points')\n",
        "ax.legend()\n",
        "\n",
        "# display the image patches\n",
        "for i, patch in enumerate(grass_patches):\n",
        "    ax = fig.add_subplot(3, len(grass_patches), len(grass_patches)*1 + i + 1)\n",
        "    ax.imshow(patch, cmap=plt.cm.gray,\n",
        "              vmin=0, vmax=255)\n",
        "    ax.set_xlabel('Region 1 %d' % (i + 1))\n",
        "\n",
        "for i, patch in enumerate(sky_patches):\n",
        "    ax = fig.add_subplot(3, len(sky_patches), len(sky_patches)*2 + i + 1)\n",
        "    ax.imshow(patch, cmap=plt.cm.gray,\n",
        "              vmin=0, vmax=255)\n",
        "    ax.set_xlabel('Region 2 %d' % (i + 1))\n",
        "\n",
        "\n",
        "# display the patches and plot\n",
        "fig.suptitle('co-occurrence matrix features', fontsize=14, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "sky_patches0 = np.mean(sky_patches[0])\n",
        "sky_patches1 = np.mean(sky_patches[1])\n",
        "sky_patches2 = np.mean(sky_patches[2])\n",
        "sky_patches3 = np.mean(sky_patches[3])\n",
        "\n",
        "Glcm_fea1 = [sky_patches0,sky_patches1,sky_patches2,sky_patches3]\n",
        "\n",
        "Testfea = [temp_m1, temp_st1, temp_vt1, temp_sk1, temp_md1,\n",
        "             temp_m2,temp_st2,temp_vt2,temp_sk2,temp_md2,\n",
        "             sky_patches0,sky_patches1,sky_patches2,sky_patches3]\n",
        "\n",
        "print('Test Feature = ',Testfea)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JX-o5rzGTNNE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import argparse\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_data1 = os.listdir('/content/drive/MyDrive/Eye dataset/close_look/')\n",
        "test_data2 = os.listdir('/content/drive/MyDrive/Eye dataset/forward_look/')\n",
        "test_data3 = os.listdir('/content/drive/MyDrive/Eye dataset/left_look/')\n",
        "test_data4 = os.listdir('/content/drive/MyDrive/Eye dataset/right_look/')\n",
        "test_data5 = os.listdir('/content/drive/MyDrive/Eye dataset/Newimage/')\n",
        "\n",
        "dot= []\n",
        "labels_target = []\n",
        "\n",
        "for img in test_data1:\n",
        "\n",
        "    try:\n",
        "        img_1 = plt.imread('/content/drive/MyDrive/Eye dataset/close_look/' + \"/\" + img)\n",
        "        img_resize = cv2.resize(img_1,((224, 224)))\n",
        "        try:\n",
        "\n",
        "\n",
        "            GRAY = img_resize[:,:,0]\n",
        "\n",
        "        except:\n",
        "            GRAY = img_resize\n",
        "\n",
        "        dot.append(np.array(GRAY))\n",
        "        labels_target.append(0)\n",
        "\n",
        "    except:\n",
        "        None\n",
        "\n",
        "for img in test_data2:\n",
        "\n",
        "    try:\n",
        "        img_2 = plt.imread('/content/drive/MyDrive/Eye dataset/forward_look/'+ \"/\" + img)\n",
        "        img_resize = cv2.resize(img_2,(224, 224))\n",
        "\n",
        "        try:\n",
        "\n",
        "\n",
        "            GRAY = img_resize[:,:,0]\n",
        "\n",
        "        except:\n",
        "            GRAY = img_resize\n",
        "\n",
        "        dot.append(np.array(GRAY))\n",
        "        labels_target.append(1)\n",
        "\n",
        "    except:\n",
        "        None\n",
        "\n",
        "for img in test_data3:\n",
        "\n",
        "    try:\n",
        "        img_3 = plt.imread('/content/drive/MyDrive/Eye dataset/left_look/'+ \"/\" + img)\n",
        "        img_resize = cv2.resize(img_3,(224, 224))\n",
        "\n",
        "        try:\n",
        "\n",
        "\n",
        "            GRAY = img_resize[:,:,0]\n",
        "\n",
        "        except:\n",
        "            GRAY = img_resize\n",
        "\n",
        "        dot.append(np.array(GRAY))\n",
        "        labels_target.append(2)\n",
        "\n",
        "    except:\n",
        "        None\n",
        "\n",
        "\n",
        "for img in test_data4:\n",
        "\n",
        "    try:\n",
        "        img_4 = plt.imread('/content/drive/MyDrive/Eye dataset/right_look/'+ \"/\" + img)\n",
        "        img_resize = cv2.resize(img_4,(224, 224))\n",
        "\n",
        "        try:\n",
        "\n",
        "\n",
        "            GRAY = img_resize[:,:,0]\n",
        "\n",
        "        except:\n",
        "            GRAY = img_resize\n",
        "\n",
        "        dot.append(np.array(GRAY))\n",
        "        labels_target.append(3)\n",
        "\n",
        "    except:\n",
        "        None\n",
        "\n",
        "for img in test_data5:\n",
        "\n",
        "    try:\n",
        "        img_5 = plt.imread('/content/drive/MyDrive/Eye dataset/Newimage/'+ \"/\" + img)\n",
        "        img_resize = cv2.resize(img_5,(224, 224))\n",
        "\n",
        "        try:\n",
        "\n",
        "\n",
        "            GRAY = img_resize[:,:,0]\n",
        "\n",
        "        except:\n",
        "            GRAY = img_resize\n",
        "\n",
        "        dot.append(np.array(GRAY))\n",
        "        labels_target.append(1)\n",
        "\n",
        "    except:\n",
        "        None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T_DdYEm1vSOg"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(dot,labels_target,test_size = 0.2, random_state = 101)\n",
        "\n",
        "x_train1=np.zeros((len(x_train),224,224))\n",
        "\n",
        "try:\n",
        "\n",
        "    for i in range(0,len(x_train)):\n",
        "            x_train1[i,:,:,:]=x_train[i]\n",
        "except:\n",
        "\n",
        "            x_train1[i,:,:]=x_train[i]\n",
        "\n",
        "x_test1=np.zeros((len(x_test),224,224))\n",
        "\n",
        "try:\n",
        "\n",
        "    for i in range(0,len(x_test)):\n",
        "            x_test1[i,:,:,:]=x_test[i]\n",
        "except:\n",
        "\n",
        "            x_test1[i,:,:]=x_test[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ChIJt1XiTb57"
      },
      "outputs": [],
      "source": [
        "from skimage.feature import graycomatrix, graycoprops\n",
        "import numpy as np\n",
        "\n",
        "Trainfea = []\n",
        "for ijk in range(0,len(labels_target)):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Load the image\n",
        "    image = dot[ijk]\n",
        "    median_filtered = cv2.medianBlur(image, 5)  # You can adjust the kernel size (5 in this case)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    temp_m1 = np.mean(image)\n",
        "    temp_st1 = np.std(image)\n",
        "    temp_vt1 = np.var(image)\n",
        "    temp_sk1 = np.mean(skew(image))\n",
        "    temp_md1 = np.median(image)\n",
        "\n",
        "    temp_m2 = np.mean(median_filtered)\n",
        "    temp_st2 = np.std(median_filtered)\n",
        "    temp_vt2 = np.var(median_filtered)\n",
        "    temp_sk2 = np.mean(skew(median_filtered))\n",
        "    temp_md2 = np.median(median_filtered)\n",
        "\n",
        "\n",
        "    # ==========================================================================\n",
        "\n",
        "    # Feature Extraction\n",
        "\n",
        "\n",
        "\n",
        "    # open the image\n",
        "\n",
        "    from skimage.feature import graycomatrix, graycoprops\n",
        "\n",
        "    PATCH_SIZE = 21\n",
        "\n",
        "    image = median_filtered\n",
        "    image = (cv2.resize(image,(768,1024))*255).astype('uint8')\n",
        "\n",
        "    # select some patches from foreground and background\n",
        "\n",
        "    grass_locations = [(280, 454), (342, 223), (444, 192), (455, 455)]\n",
        "    grass_patches = []\n",
        "    for loc in grass_locations:\n",
        "        grass_patches.append(image[loc[0]:loc[0] + PATCH_SIZE,\n",
        "                                   loc[1]:loc[1] + PATCH_SIZE])\n",
        "\n",
        "    # select some patches from sky areas of the image\n",
        "    sky_locations = [(38, 34), (139, 28), (37, 437), (145, 379)]\n",
        "    sky_patches = []\n",
        "    for loc in sky_locations:\n",
        "        sky_patches.append(image[loc[0]:loc[0] + PATCH_SIZE,\n",
        "                                 loc[1]:loc[1] + PATCH_SIZE])\n",
        "\n",
        "    # compute some GLCM properties each patch\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for patch in (grass_patches + sky_patches):\n",
        "        glcm = graycomatrix(patch, distances=[5], angles=[0], levels=256,symmetric=True)\n",
        "        xs.append(graycoprops(glcm, 'dissimilarity')[0, 0])\n",
        "        ys.append(graycoprops(glcm, 'correlation')[0, 0])\n",
        "\n",
        "    # # create the figure\n",
        "    # fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "    # # display original image with locations of patches\n",
        "    # ax = fig.add_subplot(3, 2, 1)\n",
        "    # ax.imshow(image, cmap=plt.cm.gray,\n",
        "    #           vmin=0, vmax=255)\n",
        "    # for (y, x) in grass_locations:\n",
        "    #     ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'gs')\n",
        "    # for (y, x) in sky_locations:\n",
        "    #     ax.plot(x + PATCH_SIZE / 2, y + PATCH_SIZE / 2, 'bs')\n",
        "    # ax.set_xlabel('Original Image')\n",
        "    # ax.set_xticks([])\n",
        "    # ax.set_yticks([])\n",
        "    # ax.axis('image')\n",
        "\n",
        "    # # for each patch, plot (dissimilarity, correlation)\n",
        "    # ax = fig.add_subplot(3, 2, 2)\n",
        "    # ax.plot(xs[:len(grass_patches)], ys[:len(grass_patches)], 'go',\n",
        "    #         label='Region 1')\n",
        "    # ax.plot(xs[len(grass_patches):], ys[len(grass_patches):], 'bo',\n",
        "    #         label='Region 2')\n",
        "    # ax.set_xlabel('Feature Index')\n",
        "    # ax.set_ylabel('Feature Points')\n",
        "    # ax.legend()\n",
        "\n",
        "    # # display the image patches\n",
        "    # for i, patch in enumerate(grass_patches):\n",
        "    #     ax = fig.add_subplot(3, len(grass_patches), len(grass_patches)*1 + i + 1)\n",
        "    #     ax.imshow(patch, cmap=plt.cm.gray,\n",
        "    #               vmin=0, vmax=255)\n",
        "    #     ax.set_xlabel('Region 1 %d' % (i + 1))\n",
        "\n",
        "    # for i, patch in enumerate(sky_patches):\n",
        "    #     ax = fig.add_subplot(3, len(sky_patches), len(sky_patches)*2 + i + 1)\n",
        "    #     ax.imshow(patch, cmap=plt.cm.gray,\n",
        "    #               vmin=0, vmax=255)\n",
        "    #     ax.set_xlabel('Region 2 %d' % (i + 1))\n",
        "\n",
        "\n",
        "    # # display the patches and plot\n",
        "    # fig.suptitle('co-occurrence matrix features', fontsize=14, y=1.05)\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    sky_patches0 = np.mean(sky_patches[0])\n",
        "    sky_patches1 = np.mean(sky_patches[1])\n",
        "    sky_patches2 = np.mean(sky_patches[2])\n",
        "    sky_patches3 = np.mean(sky_patches[3])\n",
        "\n",
        "    Glcm_fea1 = [sky_patches0,sky_patches1,sky_patches2,sky_patches3]\n",
        "\n",
        "    # # Calculate the GLCM\n",
        "    # d = 1  # Distance between pixels\n",
        "    # angles = [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4]  # Angles for co-occurrence matrix\n",
        "    # levels = 256  # Number of gray levels\n",
        "    # glcm = greycomatrix(image.astype('uint8'), [d], angles, levels=levels, symmetric=True, normed=True)\n",
        "\n",
        "\n",
        "\n",
        "    # # Calculate GLCM properties (example: contrast and energy)\n",
        "    # contrast = greycoprops(glcm, prop='contrast')\n",
        "\n",
        "    # energy = greycoprops(glcm, prop='energy')\n",
        "\n",
        "    # corr = greycoprops(glcm, prop='correlation')\n",
        "\n",
        "    # dissim = greycoprops(glcm, prop='dissimilarity')\n",
        "\n",
        "    # auto_correlation = np.sum(glcm[0, :, :, 0] * np.arange(levels) * np.arange(levels))\n",
        "\n",
        "    # homog = greycoprops(glcm, prop='homogeneity')\n",
        "\n",
        "    # # Calculate Sum Average\n",
        "    # sum_average = greycoprops(glcm, 'ASM')\n",
        "\n",
        "    # # Calculate Sum Variance\n",
        "    # sum_variance = greycoprops(glcm, 'contrast')\n",
        "\n",
        "    # # Calculate Sum Entropy\n",
        "    # sum_entropy = greycoprops(glcm, 'homogeneity')\n",
        "\n",
        "    # # Print or use the GLCM properties for texture analysis\n",
        "    # print(\"Contrast:\", contrast)\n",
        "    # print(\"Energy:\", energy)\n",
        "    # print(\"Correlation:\", corr)\n",
        "    # print(\"Dis-Similarity:\", dissim)\n",
        "    # print(\"Auto-Correlation:\", auto_correlation)\n",
        "    # print(\"Homoginity:\", homog)\n",
        "\n",
        "\n",
        "    # # Print or use the GLCM properties\n",
        "    # print(\"Sum Average:\", sum_average)\n",
        "    # print(\"Sum Variance:\", sum_variance)\n",
        "    # print(\"Sum Entropy:\", sum_entropy)\n",
        "\n",
        "\n",
        "\n",
        "    # TTfea = [temp_m1, temp_st1, temp_vt1, temp_sk1, temp_md1,\n",
        "    #        np.mean(contrast), np.mean(energy), np.mean(corr), np.mean(dissim)\n",
        "    #        ,np.mean(auto_correlation), np.mean(homog) ,\n",
        "    #        np.mean(sum_average) ,np.mean(sum_variance)\n",
        "    #        ,np.mean(sum_entropy),temp_m2,temp_st2,temp_vt2,temp_sk2,temp_md2]\n",
        "\n",
        "    TTfea = [temp_m1, temp_st1, temp_vt1, temp_sk1, temp_md1,\n",
        "             temp_m2,temp_st2,temp_vt2,temp_sk2,temp_md2,\n",
        "             sky_patches0,sky_patches1,sky_patches2,sky_patches3]\n",
        "    Trainfea.append(TTfea)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yvnBLjhHThwT",
        "outputId": "fec5196f-bbbe-4634-b30b-edc756353401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "------> Convolutional Neural Network  \n",
            "------------------------------------------------\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 70, 30, 16)        80        \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 35, 15, 16)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 35, 15, 32)        2080      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 17, 7, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 17, 7, 64)         8256      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 8, 3, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 3, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1536)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 500)               768500    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 500)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 2004      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 780920 (2.98 MB)\n",
            "Trainable params: 780920 (2.98 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "289/289 [==============================] - 6s 16ms/step - loss: 0.6719 - accuracy: 0.2612\n",
            "Epoch 2/10\n",
            "289/289 [==============================] - 5s 16ms/step - loss: 0.6376 - accuracy: 0.2768\n",
            "Epoch 3/10\n",
            "289/289 [==============================] - 4s 13ms/step - loss: 0.6132 - accuracy: 0.2768\n",
            "Epoch 4/10\n",
            "289/289 [==============================] - 4s 14ms/step - loss: 0.5960 - accuracy: 0.2907\n",
            "Epoch 5/10\n",
            "289/289 [==============================] - 5s 17ms/step - loss: 0.5838 - accuracy: 0.2907\n",
            "Epoch 6/10\n",
            "289/289 [==============================] - 4s 13ms/step - loss: 0.5752 - accuracy: 0.2907\n",
            "Epoch 7/10\n",
            "289/289 [==============================] - 4s 13ms/step - loss: 0.5692 - accuracy: 0.2907\n",
            "Epoch 8/10\n",
            "289/289 [==============================] - 5s 17ms/step - loss: 0.5649 - accuracy: 0.2907\n",
            "Epoch 9/10\n",
            "289/289 [==============================] - 4s 14ms/step - loss: 0.5619 - accuracy: 0.2907\n",
            "Epoch 10/10\n",
            "289/289 [==============================] - 4s 13ms/step - loss: 0.5597 - accuracy: 0.2907\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5587 - accuracy: 0.2345\n",
            "Accuracy: 1.0\n",
            "[1]\n",
            "----------- Classification Result --------------\n",
            "Class Value = 1\n",
            "-------------------\n",
            "Identified ---\n",
            "Anemia\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "y_train1=np.array(y_train)\n",
        "y_test1=np.array(y_test)\n",
        "\n",
        "train_Y_one_hot = to_categorical(y_train1)\n",
        "test_Y_one_hot = to_categorical(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train2=np.zeros((len(x_train),70,30))\n",
        "for i in range(0,len(x_train)):\n",
        "        x_train2[i,:,:]=x_train2[i]\n",
        "\n",
        "x_test2=np.zeros((len(x_test),70,30))\n",
        "for i in range(0,len(x_test)):\n",
        "        x_test2[i,:,:]=x_test2[i]\n",
        "\n",
        "\n",
        "\n",
        "from keras.layers import Dense, Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "print(\"------> Convolutional Neural Network  \")\n",
        "print(\"------------------------------------------------\")\n",
        "print()\n",
        "\n",
        "# initialize the model\n",
        "model=Sequential()\n",
        "\n",
        "\n",
        "#CNN layes\n",
        "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(70,30,1)))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(500,activation=\"relu\"))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(4,activation=\"softmax\"))\n",
        "\n",
        "#summary the model\n",
        "model.summary()\n",
        "\n",
        "#compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "y_train1=np.array(y_train)\n",
        "\n",
        "train_Y_one_hot = to_categorical(y_train1)\n",
        "test_Y_one_hot = to_categorical(y_test)\n",
        "\n",
        "#fit the model\n",
        "history=model.fit(x_train2,train_Y_one_hot,batch_size=2,epochs=10,verbose=1)\n",
        "accuracy = model.evaluate(x_test2, test_Y_one_hot, verbose=1)\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset (Iris dataset in this example)\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "\n",
        "# Create a DecisionTreeClassifier\n",
        "clf_dt = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf_dt.fit(Trainfea, labels_target)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_dt = clf_dt.predict([Testfea])\n",
        "\n",
        "y_pred_dt_class = clf_dt.predict(Trainfea)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy_dt = accuracy_score(labels_target, y_pred_dt_class)\n",
        "print(\"Accuracy:\", accuracy_dt)\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "clf.fit(Trainfea, labels_target)\n",
        "print(clf.predict([Testfea]))\n",
        "Class = clf.predict([Testfea])\n",
        "\n",
        "Class_rf_pred = clf.predict(Trainfea)\n",
        "\n",
        "Accuracy_RF = 100 - accuracy_score(labels_target,Class_rf_pred)\n",
        "\n",
        "print('----------- Classification Result --------------')\n",
        "print('Class Value =',str(int(Class)))\n",
        "\n",
        "\n",
        "\n",
        "if int(Class) == 0:\n",
        "    print('-------------------')\n",
        "    print('Identified ---')\n",
        "    print('Not Anemia')\n",
        "    print('-------------------')\n",
        "\n",
        "elif int(Class) == 1:\n",
        "    print('-------------------')\n",
        "    print('Identified ---')\n",
        "    print('Anemia')\n",
        "    print('-------------------')\n",
        "\n",
        "elif int(Class) == 2:\n",
        "    print('-------------------')\n",
        "    print('Identified ---')\n",
        "    print('Anemia')\n",
        "    print('-------------------')\n",
        "\n",
        "elif int(Class) == 3:\n",
        "    print('-------------------')\n",
        "    print('Identified ---')\n",
        "    print('Partially-Anemia')\n",
        "    print('-------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wLYKUolRGrWV",
        "outputId": "0455ebab-a80a-4852-aebb-293e592a3a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset (Iris dataset in this example)\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "\n",
        "# Create a DecisionTreeClassifier\n",
        "clf_dt = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf_dt.fit(Trainfea, labels_target)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_dt = clf_dt.predict([Testfea])\n",
        "\n",
        "y_pred_dt_class = clf_dt.predict(Trainfea)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy_dt = accuracy_score(labels_target, y_pred_dt_class)\n",
        "print(\"Accuracy:\", accuracy_dt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JR91S7HGGuMW",
        "outputId": "76b91686-5913-4c5d-ce64-5c91bd5e855e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]\n"
          ]
        }
      ],
      "source": [
        "# Random Forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "clf.fit(Trainfea, labels_target)\n",
        "print(clf.predict([Testfea]))\n",
        "Class = clf.predict([Testfea])\n",
        "\n",
        "Class_rf_pred = clf.predict(Trainfea)\n",
        "\n",
        "Accuracy_RF = 100 - accuracy_score(labels_target,Class_rf_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N6NgtWsdTsJ_",
        "outputId": "fc660823-ac6f-45cd-ef00-2f2e31a72f32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance Analysis\n",
            "\n",
            "1. CNN Accuracy = 99.44127494096756 %\n",
            "\n",
            "2. CNN Error Rate = 0.5587250590324402\n",
            "\n",
            "3. Accuracy of DT Algorithm =  100.0  %\n",
            "\n",
            "4. Error Rate of DT Algorithm =  99.0  %\n",
            "\n",
            "5. Precision of DT Algorithm =  100.0  %\n",
            "\n",
            "6. Recall Rate of DT Algorithm =  100.0  %\n",
            "\n",
            "7. Accuracy of RF Algorithm =  99.16459197786999  %\n",
            "\n",
            "8. Error Rate of RF Algorithm =  0.8354080221300109  %\n",
            "\n",
            "9. Accuracy of RF Algorithm =  83.50357649773642  %\n",
            "\n",
            "10. Error Rate of RF Algorithm =  83.54080221300138  %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# y_pred = ensemble.predict(Trainfea)\n",
        "Class = clf.predict(Trainfea)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Accuracy_Hybrid = 100 - accuracy_score(labels_target,y_pred)\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "# Calculate precision and recall\n",
        "precision_dt = precision_score(labels_target, y_pred_dt_class,average='weighted')\n",
        "recall_dt = recall_score(labels_target, y_pred_dt_class,average='weighted')\n",
        "\n",
        "precision_rf = precision_score(labels_target, Class_rf_pred,average='weighted')\n",
        "recall_rf = recall_score(labels_target, Class_rf_pred,average='weighted')\n",
        "\n",
        "print(\"Performance Analysis\")\n",
        "print()\n",
        "error = accuracy[0]\n",
        "acc_cnn=100-error\n",
        "\n",
        "\n",
        "print(\"1. CNN Accuracy =\",acc_cnn,'%')\n",
        "print()\n",
        "error=100-acc_cnn\n",
        "print(\"2. CNN Error Rate =\",error)\n",
        "print()\n",
        "\n",
        "print('3. Accuracy of DT Algorithm = ',accuracy_dt*100,' %')\n",
        "print()\n",
        "print('4. Error Rate of DT Algorithm = ',100-accuracy_dt,' %')\n",
        "print()\n",
        "\n",
        "print('5. Precision of DT Algorithm = ',precision_dt*100,' %')\n",
        "print()\n",
        "print('6. Recall Rate of DT Algorithm = ',recall_dt*100,' %')\n",
        "print()\n",
        "\n",
        "print('7. Accuracy of RF Algorithm = ',Accuracy_RF,' %')\n",
        "print()\n",
        "print('8. Error Rate of RF Algorithm = ',100-Accuracy_RF,' %')\n",
        "print()\n",
        "\n",
        "print('9. Accuracy of RF Algorithm = ',precision_rf*100,' %')\n",
        "print()\n",
        "print('10. Error Rate of RF Algorithm = ',recall_rf*100,' %')\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UQ0GaNKiCY68",
        "outputId": "10127a8c-dcf7-4e6b-8b41-526b8c7e3ef9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>            </th><th>Model      </th><th style=\"text-align: right;\">    Metric</th><th>Value  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>CNN         </td><td>Accuracy   </td><td style=\"text-align: right;\"> 99.4413  </td><td>%      </td></tr>\n",
              "<tr><td>CNN         </td><td>Error Rate </td><td style=\"text-align: right;\">  0.558725</td><td>%      </td></tr>\n",
              "<tr><td>DT Algorithm</td><td>Accuracy   </td><td style=\"text-align: right;\">100       </td><td>%      </td></tr>\n",
              "<tr><td>DT Algorithm</td><td>Error Rate </td><td style=\"text-align: right;\"> 99       </td><td>%      </td></tr>\n",
              "<tr><td>DT Algorithm</td><td>Precision  </td><td style=\"text-align: right;\">100       </td><td>%      </td></tr>\n",
              "<tr><td>DT Algorithm</td><td>Recall Rate</td><td style=\"text-align: right;\">100       </td><td>%      </td></tr>\n",
              "<tr><td>RF Algorithm</td><td>Accuracy   </td><td style=\"text-align: right;\"> 99.1646  </td><td>%      </td></tr>\n",
              "<tr><td>RF Algorithm</td><td>Error Rate </td><td style=\"text-align: right;\">  0.835408</td><td>%      </td></tr>\n",
              "<tr><td>RF Algorithm</td><td>Precision  </td><td style=\"text-align: right;\"> 83.5036  </td><td>%      </td></tr>\n",
              "<tr><td>RF Algorithm</td><td>Recall Rate</td><td style=\"text-align: right;\"> 83.5408  </td><td>%      </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Performance Metrics\n",
        "metrics_data = [\n",
        "    [\"CNN\", \"Accuracy\", acc_cnn,'%'],\n",
        "    [\"CNN\", \"Error Rate\", error,'%'],\n",
        "    [\"DT Algorithm\", \"Accuracy\", accuracy_dt*100,' %'],\n",
        "    [\"DT Algorithm\", \"Error Rate\", 100-accuracy_dt,' %'],\n",
        "    [\"DT Algorithm\", \"Precision\", precision_dt*100,' %'],\n",
        "    [\"DT Algorithm\", \"Recall Rate\", recall_dt*100,' %'],\n",
        "    [\"RF Algorithm\", \"Accuracy\", Accuracy_RF,' %'],\n",
        "    [\"RF Algorithm\", \"Error Rate\", 100-Accuracy_RF,' %'],\n",
        "    [\"RF Algorithm\", \"Precision\", precision_rf*100,' %'],\n",
        "    [\"RF Algorithm\", \"Recall Rate\", recall_rf*100,' %'],\n",
        "]\n",
        "\n",
        "# Create a table\n",
        "table = tabulate(metrics_data, headers=[\"Model\", \"Metric\", \"Value\"], tablefmt=\"html\")\n",
        "\n",
        "# Display the table in Colab\n",
        "display(HTML(table))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l1BzQ9Y3I9CK"
      },
      "outputs": [],
      "source": [
        "#Comparission\n",
        "\n",
        "#The Decision Tree (DT) algorithm exhibits impressive accuracy of 100%, with a minute error rate of 1%. This model showcases exceptional precision and recall rates, both at 100%, indicating its robustness in correctly identifying positive samples while minimizing false positives and false negatives. On the other hand, the Random Forest (RF) algorithm, while maintaining a commendable accuracy of 99.16%, experiences a higher error rate at 0.84%.\n",
        "#The RF algorithm demonstrates good precision and recall rates at approximately 84%, indicating a balanced but slightly lower ability to accurately classify samples compared to the DT algorithm.\n",
        "#Overall, the DT algorithm excels in precision and recall, while the RF algorithm strikes a balance between accuracy and error rates."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}